{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUrk1Imc/6hRv1ALk6RBPX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"c9vGTeMuEfhP"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","\n","np.random.seed(42)\n","\n","# Activation functions\n","def relu(x): return np.maximum(0, x)\n","def relu_derivative(x): return (x > 0).astype(float)\n","\n","def sigmoid(x): return 1 / (1 + np.exp(-x))\n","def sigmoid_derivative(x): return sigmoid(x) * (1 - sigmoid(x))\n","\n","def tanh(x): return np.tanh(x)\n","def tanh_derivative(x): return 1 - np.tanh(x)**2\n","\n","# Deep ANN class\n","class DeepNeuralNetwork:\n","    def __init__(self, layer_sizes, activation='relu'):\n","        self.num_layers = len(layer_sizes) - 1\n","        self.activation_name = activation\n","        self.params = {}\n","\n","        # Activation function\n","        if activation == 'relu':\n","            self.act = relu\n","            self.act_deriv = relu_derivative\n","            init_factor = lambda n: np.sqrt(2. / n)\n","        elif activation == 'sigmoid':\n","            self.act = sigmoid\n","            self.act_deriv = sigmoid_derivative\n","            init_factor = lambda n: np.sqrt(1. / n)\n","        elif activation == 'tanh':\n","            self.act = tanh\n","            self.act_deriv = tanh_derivative\n","            init_factor = lambda n: np.sqrt(1. / n)\n","        else:\n","            raise ValueError(\"Unsupported activation\")\n","\n","        # Initialize weights and biases\n","        for i in range(self.num_layers):\n","            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n","            self.params[f'W{i+1}'] = np.random.randn(n_in, n_out) * init_factor(n_in)\n","            self.params[f'b{i+1}'] = np.zeros((1, n_out))\n","\n","    def softmax(self, x):\n","        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n","        return exps / np.sum(exps, axis=1, keepdims=True)\n","\n","    def forward(self, X):\n","        self.cache = {'A0': X}\n","        for i in range(1, self.num_layers):\n","            Z = self.cache[f'A{i-1}'] @ self.params[f'W{i}'] + self.params[f'b{i}']\n","            A = self.act(Z)\n","            self.cache[f'Z{i}'] = Z\n","            self.cache[f'A{i}'] = A\n","        # Output layer (softmax)\n","        Z = self.cache[f'A{self.num_layers-1}'] @ self.params[f'W{self.num_layers}'] + self.params[f'b{self.num_layers}']\n","        A = self.softmax(Z)\n","        self.cache[f'Z{self.num_layers}'] = Z\n","        self.cache[f'A{self.num_layers}'] = A\n","        return A\n","\n","    def backward(self, X, y, lr):\n","        m = X.shape[0]\n","        L = self.num_layers\n","        dZ = self.cache[f'A{L}'] - y\n","\n","        for i in reversed(range(1, L + 1)):\n","            dW = self.cache[f'A{i-1}'].T @ dZ / m\n","            db = np.sum(dZ, axis=0, keepdims=True) / m\n","\n","            self.params[f'W{i}'] -= lr * dW\n","            self.params[f'b{i}'] -= lr * db\n","\n","            if i > 1:\n","                dA_prev = dZ @ self.params[f'W{i}'].T\n","                dZ = dA_prev * self.act_deriv(self.cache[f'Z{i-1}'])\n","\n","    def train(self, X, y, epochs=1000, lr=0.1):\n","        for epoch in range(epochs):\n","            output = self.forward(X)\n","            loss = -np.mean(np.sum(y * np.log(output + 1e-9), axis=1))\n","            self.backward(X, y, lr)\n","            if epoch % 100 == 0:\n","                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n","\n","    def predict(self, X):\n","        output = self.forward(X)\n","        return np.argmax(output, axis=1)"]},{"cell_type":"code","source":["# Load dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target.reshape(-1, 1)\n","\n","# One-hot encode labels\n","encoder = OneHotEncoder(sparse_output=False)\n","y_encoded = encoder.fit_transform(y)\n","\n","# Normalize features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)"],"metadata":{"id":"NZOVGrqgFEpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a deep architecture: 4 hidden layers\n","layer_sizes = [4, 32, 64, 32, 16, 3]  # Input:4, Hidden:4 layers, Output:3\n","\n","# Try different activations\n","for activation in ['relu', 'tanh', 'sigmoid']:\n","    print(f\"\\nTraining with activation: {activation.upper()}\")\n","    model = DeepNeuralNetwork(layer_sizes, activation=activation)\n","    model.train(X_train, y_train, epochs=1000, lr=0.05)\n","    preds = model.predict(X_test)\n","    accuracy = np.mean(np.argmax(y_test, axis=1) == preds)\n","    print(f\"Test Accuracy with {activation.upper()}: {accuracy * 100:.2f}%\")\n"],"metadata":{"id":"HCZLy5GtFJHO","executionInfo":{"status":"ok","timestamp":1750955510060,"user_tz":-330,"elapsed":4066,"user":{"displayName":"W DUNRINGMUI MAKUNGA","userId":"08486257095691583827"}},"outputId":"35abafb5-6c1c-4b99-c6a6-9e7987ee9dde","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training with activation: RELU\n","Epoch 0, Loss: 1.4050\n","Epoch 100, Loss: 0.1377\n","Epoch 200, Loss: 0.0835\n","Epoch 300, Loss: 0.0616\n","Epoch 400, Loss: 0.0457\n","Epoch 500, Loss: 0.0361\n","Epoch 600, Loss: 0.0289\n","Epoch 700, Loss: 0.0232\n","Epoch 800, Loss: 0.0184\n","Epoch 900, Loss: 0.0140\n","Test Accuracy with RELU: 100.00%\n","\n","Training with activation: TANH\n","Epoch 0, Loss: 1.1415\n","Epoch 100, Loss: 0.2430\n","Epoch 200, Loss: 0.1286\n","Epoch 300, Loss: 0.0897\n","Epoch 400, Loss: 0.0714\n","Epoch 500, Loss: 0.0625\n","Epoch 600, Loss: 0.0572\n","Epoch 700, Loss: 0.0535\n","Epoch 800, Loss: 0.0505\n","Epoch 900, Loss: 0.0478\n","Test Accuracy with TANH: 97.78%\n","\n","Training with activation: SIGMOID\n","Epoch 0, Loss: 1.1887\n","Epoch 100, Loss: 1.0961\n","Epoch 200, Loss: 1.0954\n","Epoch 300, Loss: 1.0947\n","Epoch 400, Loss: 1.0940\n","Epoch 500, Loss: 1.0933\n","Epoch 600, Loss: 1.0924\n","Epoch 700, Loss: 1.0915\n","Epoch 800, Loss: 1.0904\n","Epoch 900, Loss: 1.0892\n","Test Accuracy with SIGMOID: 35.56%\n"]}]}]}