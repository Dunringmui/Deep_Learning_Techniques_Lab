{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlRt7Q5pOyUhd0ZPx9pfkW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","\n","np.random.seed(42)  # for reproducibility\n","\n","# Activation functions and their derivatives\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    s = sigmoid(x)\n","    return s * (1 - s)\n","\n","def tanh(x):\n","    return np.tanh(x)\n","\n","def tanh_derivative(x):\n","    return 1 - np.tanh(x) ** 2\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return (x > 0).astype(float)\n","\n","# Neural Network class\n","class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n","        self.W1 = np.random.randn(input_size, hidden_size)\n","        self.b1 = np.zeros((1, hidden_size))\n","        self.W2 = np.random.randn(hidden_size, output_size)\n","        self.b2 = np.zeros((1, output_size))\n","\n","        if activation == 'sigmoid':\n","            self.activation = sigmoid\n","            self.activation_deriv = sigmoid_derivative\n","        elif activation == 'tanh':\n","            self.activation = tanh\n","            self.activation_deriv = tanh_derivative\n","        elif activation == 'relu':\n","            self.activation = relu\n","            self.activation_deriv = relu_derivative\n","        else:\n","            raise ValueError(\"Unsupported activation\")\n","\n","    def forward(self, X):\n","        self.Z1 = X @ self.W1 + self.b1\n","        self.A1 = self.activation(self.Z1)\n","        self.Z2 = self.A1 @ self.W2 + self.b2\n","        self.A2 = self.softmax(self.Z2)\n","        return self.A2\n","\n","    def softmax(self, x):\n","        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n","        return exps / np.sum(exps, axis=1, keepdims=True)\n","\n","    def backward(self, X, y, output, lr):\n","        m = y.shape[0]\n","        dZ2 = output - y\n","        dW2 = self.A1.T @ dZ2 / m\n","        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n","\n","        dA1 = dZ2 @ self.W2.T\n","        dZ1 = dA1 * self.activation_deriv(self.Z1)\n","        dW1 = X.T @ dZ1 / m\n","        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n","\n","        self.W1 -= lr * dW1\n","        self.b1 -= lr * db1\n","        self.W2 -= lr * dW2\n","        self.b2 -= lr * db2\n","\n","    def train(self, X, y, epochs=1000, lr=0.1):\n","        for i in range(epochs):\n","            output = self.forward(X)\n","            loss = -np.mean(np.sum(y * np.log(output + 1e-9), axis=1))\n","            self.backward(X, y, output, lr)\n","            if i % 100 == 0:\n","                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n","\n","    def predict(self, X):\n","        output = self.forward(X)\n","        return np.argmax(output, axis=1)\n","\n","# Load and preprocess Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target.reshape(-1, 1)\n","\n","# One-hot encode labels\n","encoder = OneHotEncoder(sparse_output=False)\n","y_encoded = encoder.fit_transform(y)\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)\n","\n","# Train with different activation functions\n","for act in ['sigmoid', 'tanh', 'relu']:\n","    print(f\"\\nTraining with activation function: {act.upper()}\")\n","    nn = NeuralNetwork(input_size=4, hidden_size=10, output_size=3, activation=act)\n","    nn.train(X_train, y_train, epochs=1000, lr=0.1)\n","    predictions = nn.predict(X_test)\n","    accuracy = np.mean(np.argmax(y_test, axis=1) == predictions)\n","    print(f\"Accuracy with {act.upper()}: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zV46prvIDI-P","executionInfo":{"status":"ok","timestamp":1750955280494,"user_tz":-330,"elapsed":485,"user":{"displayName":"W DUNRINGMUI MAKUNGA","userId":"08486257095691583827"}},"outputId":"ecfaf04a-0199-4332-ab20-8b5bd5fbf19d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training with activation function: SIGMOID\n","Epoch 0, Loss: 1.2620\n","Epoch 100, Loss: 0.3924\n","Epoch 200, Loss: 0.3132\n","Epoch 300, Loss: 0.2707\n","Epoch 400, Loss: 0.2398\n","Epoch 500, Loss: 0.2147\n","Epoch 600, Loss: 0.1935\n","Epoch 700, Loss: 0.1755\n","Epoch 800, Loss: 0.1603\n","Epoch 900, Loss: 0.1476\n","Accuracy with SIGMOID: 100.00%\n","\n","Training with activation function: TANH\n","Epoch 0, Loss: 3.0807\n","Epoch 100, Loss: 0.1622\n","Epoch 200, Loss: 0.1147\n","Epoch 300, Loss: 0.0942\n","Epoch 400, Loss: 0.0827\n","Epoch 500, Loss: 0.0753\n","Epoch 600, Loss: 0.0701\n","Epoch 700, Loss: 0.0663\n","Epoch 800, Loss: 0.0634\n","Epoch 900, Loss: 0.0611\n","Accuracy with TANH: 100.00%\n","\n","Training with activation function: RELU\n","Epoch 0, Loss: 1.3143\n","Epoch 100, Loss: 0.2075\n","Epoch 200, Loss: 0.1396\n","Epoch 300, Loss: 0.1072\n","Epoch 400, Loss: 0.0877\n","Epoch 500, Loss: 0.0756\n","Epoch 600, Loss: 0.0684\n","Epoch 700, Loss: 0.0628\n","Epoch 800, Loss: 0.0584\n","Epoch 900, Loss: 0.0548\n","Accuracy with RELU: 97.78%\n"]}]}]}